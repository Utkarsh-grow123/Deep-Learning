{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeNet5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhSB6wBw2B3ifYzGH+TdeV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utkarsh-grow123/Deep-Learning/blob/main/LeNet5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code with initial layer with standard normalization and hidden layers with batch normalization"
      ],
      "metadata": {
        "id": "yvsKc5hPs8r4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8I1Fc1stBLIy"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Module\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class Model(Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
        "\n",
        "        \n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, padding=0, stride=2)\n",
        "        \n",
        "        self.conv2=nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, padding=0, stride=2)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.bn3 = nn.BatchNorm1d(120)\n",
        "        self.bn4 = nn.BatchNorm1d(84)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.relu1(y)\n",
        "        y = self.pool1(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.bn2(y)\n",
        "        y = self.relu2(y)\n",
        "        y = self.pool2(y)\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        y = self.fc1(y)\n",
        "        y = self.bn3(y)\n",
        "        y = self.relu3(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.bn4(y)\n",
        "        y = self.relu4(y)\n",
        "        y = self.fc3(y)\n",
        "        y = self.relu5(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.datasets import mnist\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.2856], [0.3385]),\n",
        "])\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.2856], [0.3385]),\n",
        "])\n",
        "\n",
        "    trainset = mnist.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = mnist.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    model = Model()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "    epoch = 100\n",
        "\n",
        "    for _epoch in range(epoch):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss += loss.item()\n",
        "          _, predicted = outputs.max(1)\n",
        "          total += targets.size(0)\n",
        "          correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "      \n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct1 = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "          for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "              inputs, targets = inputs.to(device), targets.to(device)\n",
        "              loss = criterion(outputs, targets)\n",
        "\n",
        "              test_loss += loss.item()\n",
        "              _, predicted = outputs.max(1)\n",
        "              total += targets.size(0)\n",
        "              correct1 += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print('accuracy: {:.2f}'.format(correct1*100 / total))"
      ],
      "metadata": {
        "id": "8CYD793kBX_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code with input layer with batch normalisation as well as the hidden layers with batch normalisation"
      ],
      "metadata": {
        "id": "-NV6mmt8uRet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Module\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class ModelBatch(Module):\n",
        "    def __init__(self):\n",
        "        super(ModelBatch, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
        "\n",
        "        self.bn1=nn.BatchNorm2d(6)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, padding=0, stride=2)\n",
        "        \n",
        "        self.conv2=nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, padding=0, stride=2)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.bn3 = nn.BatchNorm1d(120)\n",
        "        self.bn4 = nn.BatchNorm1d(84)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.bn1(y)\n",
        "        y = self.relu1(y)\n",
        "        y = self.pool1(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.bn2(y)\n",
        "        y = self.relu2(y)\n",
        "        y = self.pool2(y)\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        y = self.fc1(y)\n",
        "        y = self.bn3(y)\n",
        "        y = self.relu3(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.bn4(y)\n",
        "        y = self.relu4(y)\n",
        "        y = self.fc3(y)\n",
        "        y = self.relu5(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "8yzuhQhbuSix"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.datasets import mnist\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "    trainset = mnist.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = mnist.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    model = ModelBatch()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "    epoch = 100\n",
        "\n",
        "    for _epoch in range(epoch):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss += loss.item()\n",
        "          _, predicted = outputs.max(1)\n",
        "          total += targets.size(0)\n",
        "          correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "      \n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct1 = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "          for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "              inputs, targets = inputs.to(device), targets.to(device)\n",
        "              loss = criterion(outputs, targets)\n",
        "\n",
        "              test_loss += loss.item()\n",
        "              _, predicted = outputs.max(1)\n",
        "              total += targets.size(0)\n",
        "              correct1 += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print('accuracy: {:.2f}'.format(correct1*100 / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "wKcFe5KTuhB2",
        "outputId": "8c6eb727-0868-438a-f45c-cab70f12d970"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 9.98\n",
            "accuracy: 10.46\n",
            "accuracy: 9.46\n",
            "accuracy: 9.92\n",
            "accuracy: 10.10\n",
            "accuracy: 9.88\n",
            "accuracy: 9.77\n",
            "accuracy: 9.92\n",
            "accuracy: 9.68\n",
            "accuracy: 10.31\n",
            "accuracy: 10.20\n",
            "accuracy: 9.56\n",
            "accuracy: 10.03\n",
            "accuracy: 10.50\n",
            "accuracy: 9.60\n",
            "accuracy: 10.14\n",
            "accuracy: 10.03\n",
            "accuracy: 10.14\n",
            "accuracy: 9.67\n",
            "accuracy: 9.97\n",
            "accuracy: 9.97\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-72d67f2023c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8W4RO8Peugdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code with dropout layer and wothout standard or batch normalization\n"
      ],
      "metadata": {
        "id": "ysJ2lv7RxrYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Module\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class ModelDropout(Module):\n",
        "    def __init__(self):\n",
        "        super(ModelDropout, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
        "\n",
        "        \n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, padding=0, stride=2)\n",
        "        \n",
        "        self.conv2=nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropoutConv = nn.Dropout(0.2)\n",
        "        self.dropoutHidden = nn.Dropout(0.5)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, padding=0, stride=2)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu5 = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.dropoutConv(y)\n",
        "        y = self.relu1(y)\n",
        "        y = self.pool1(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.dropoutHidden(y)\n",
        "        y = self.relu2(y)\n",
        "        y = self.pool2(y)\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        y = self.fc1(y)\n",
        "        y = self.dropoutHidden(y)\n",
        "        y = self.relu3(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.dropoutHidden(y)\n",
        "        y = self.relu4(y)\n",
        "        y = self.fc3(y)\n",
        "        y = self.relu5(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "CQP5P1KaxpXB"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.datasets import mnist\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "    trainset = mnist.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = mnist.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    model = ModelDropout()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "    epoch = 100\n",
        "\n",
        "    for _epoch in range(epoch):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss += loss.item()\n",
        "          _, predicted = outputs.max(1)\n",
        "          total += targets.size(0)\n",
        "          correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "      \n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct1 = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "          for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "              inputs, targets = inputs.to(device), targets.to(device)\n",
        "              loss = criterion(outputs, targets)\n",
        "\n",
        "              test_loss += loss.item()\n",
        "              _, predicted = outputs.max(1)\n",
        "              total += targets.size(0)\n",
        "              correct1 += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print('accuracy: {:.2f}'.format(correct1*100 / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q6QMx-yx9Q5",
        "outputId": "13f60294-3d8a-489b-880d-523a468c1b93"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 10.01\n",
            "accuracy: 9.77\n",
            "accuracy: 9.89\n",
            "accuracy: 9.52\n",
            "accuracy: 10.04\n",
            "accuracy: 10.10\n",
            "accuracy: 9.82\n",
            "accuracy: 9.92\n",
            "accuracy: 10.28\n",
            "accuracy: 10.43\n",
            "accuracy: 9.84\n",
            "accuracy: 10.06\n",
            "accuracy: 10.22\n",
            "accuracy: 10.42\n",
            "accuracy: 9.73\n",
            "accuracy: 9.96\n",
            "accuracy: 9.51\n",
            "accuracy: 10.20\n",
            "accuracy: 10.02\n",
            "accuracy: 10.47\n",
            "accuracy: 10.21\n",
            "accuracy: 9.36\n",
            "accuracy: 10.21\n",
            "accuracy: 9.87\n",
            "accuracy: 10.25\n",
            "accuracy: 10.01\n",
            "accuracy: 10.03\n",
            "accuracy: 10.13\n",
            "accuracy: 9.56\n",
            "accuracy: 9.91\n",
            "accuracy: 10.45\n",
            "accuracy: 10.23\n",
            "accuracy: 10.05\n",
            "accuracy: 9.96\n",
            "accuracy: 10.06\n",
            "accuracy: 10.12\n",
            "accuracy: 9.93\n",
            "accuracy: 10.44\n",
            "accuracy: 9.46\n",
            "accuracy: 9.80\n",
            "accuracy: 9.67\n",
            "accuracy: 10.10\n",
            "accuracy: 10.06\n",
            "accuracy: 9.64\n",
            "accuracy: 10.22\n",
            "accuracy: 9.92\n",
            "accuracy: 9.67\n",
            "accuracy: 9.79\n",
            "accuracy: 9.92\n",
            "accuracy: 9.94\n",
            "accuracy: 9.76\n",
            "accuracy: 9.97\n",
            "accuracy: 10.04\n",
            "accuracy: 9.78\n",
            "accuracy: 9.92\n",
            "accuracy: 9.95\n",
            "accuracy: 9.60\n",
            "accuracy: 9.83\n",
            "accuracy: 10.67\n",
            "accuracy: 9.84\n",
            "accuracy: 9.88\n",
            "accuracy: 10.23\n",
            "accuracy: 10.06\n",
            "accuracy: 10.16\n",
            "accuracy: 10.25\n",
            "accuracy: 10.00\n",
            "accuracy: 10.24\n",
            "accuracy: 9.86\n",
            "accuracy: 9.89\n",
            "accuracy: 9.78\n",
            "accuracy: 9.62\n",
            "accuracy: 9.80\n",
            "accuracy: 10.04\n",
            "accuracy: 9.57\n",
            "accuracy: 10.10\n",
            "accuracy: 10.28\n",
            "accuracy: 10.03\n",
            "accuracy: 10.24\n",
            "accuracy: 9.99\n",
            "accuracy: 9.95\n",
            "accuracy: 10.18\n",
            "accuracy: 9.83\n",
            "accuracy: 9.80\n",
            "accuracy: 10.38\n",
            "accuracy: 9.25\n",
            "accuracy: 10.31\n",
            "accuracy: 10.25\n",
            "accuracy: 10.77\n",
            "accuracy: 9.95\n",
            "accuracy: 10.24\n",
            "accuracy: 9.88\n",
            "accuracy: 9.70\n",
            "accuracy: 10.02\n",
            "accuracy: 10.16\n",
            "accuracy: 10.16\n",
            "accuracy: 10.33\n",
            "accuracy: 9.80\n",
            "accuracy: 9.50\n",
            "accuracy: 10.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code with both dropout and batch normalization"
      ],
      "metadata": {
        "id": "tqWHI9vYArMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Module\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class ModelBatch(Module):\n",
        "    def __init__(self):\n",
        "        super(ModelBatch, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
        "\n",
        "        self.bn1=nn.BatchNorm2d(6)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, padding=0, stride=2)\n",
        "        self.dropoutConv = nn.Dropout(0.2)\n",
        "        self.dropoutHidden = nn.Dropout(0.5)\n",
        "        self.conv2=nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, padding=0, stride=2)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.bn3 = nn.BatchNorm1d(120)\n",
        "        self.bn4 = nn.BatchNorm1d(84)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.dropoutConv(y)\n",
        "        y = self.bn1(y)\n",
        "        y = self.relu1(y)\n",
        "        y = self.pool1(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.dropoutHidden(y)\n",
        "        y = self.bn2(y)\n",
        "        y = self.relu2(y)\n",
        "        y = self.pool2(y)\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        y = self.fc1(y)\n",
        "        y = self.dropoutHidden(y)\n",
        "        y = self.bn3(y)\n",
        "        y = self.relu3(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.dropoutHidden(y)\n",
        "        y = self.bn4(y)\n",
        "        y = self.relu4(y)\n",
        "        y = self.fc3(y)\n",
        "        y = self.relu5(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "z1KmrZMlA3nL"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.datasets import mnist\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "    trainset = mnist.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = mnist.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    model = ModelDropout()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "    epoch = 100\n",
        "\n",
        "    for _epoch in range(epoch):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss += loss.item()\n",
        "          _, predicted = outputs.max(1)\n",
        "          total += targets.size(0)\n",
        "          correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "      \n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct1 = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "          for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "              inputs, targets = inputs.to(device), targets.to(device)\n",
        "              loss = criterion(outputs, targets)\n",
        "\n",
        "              test_loss += loss.item()\n",
        "              _, predicted = outputs.max(1)\n",
        "              total += targets.size(0)\n",
        "              correct1 += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print('accuracy: {:.2f}'.format(correct1*100 / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0Mma4x8CmzO",
        "outputId": "680f8c62-a30b-4c0e-aa35-914850732c05"
      },
      "execution_count": 41,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 10.45\n",
            "accuracy: 9.89\n",
            "accuracy: 10.21\n",
            "accuracy: 10.24\n",
            "accuracy: 10.37\n",
            "accuracy: 9.93\n",
            "accuracy: 10.22\n",
            "accuracy: 9.42\n",
            "accuracy: 9.87\n",
            "accuracy: 9.95\n",
            "accuracy: 9.72\n",
            "accuracy: 9.62\n",
            "accuracy: 9.90\n",
            "accuracy: 10.26\n",
            "accuracy: 9.78\n",
            "accuracy: 10.45\n",
            "accuracy: 10.24\n",
            "accuracy: 9.74\n",
            "accuracy: 9.76\n",
            "accuracy: 10.14\n",
            "accuracy: 10.26\n",
            "accuracy: 10.27\n",
            "accuracy: 10.30\n",
            "accuracy: 9.45\n",
            "accuracy: 9.94\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 9.63\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 9.65\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 10.17\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 10.44\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 9.75\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "    if w.is_alive():\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    self._shutdown_workers()\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7fb69c7050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 10.70\n",
            "accuracy: 10.64\n",
            "accuracy: 10.09\n",
            "accuracy: 10.49\n",
            "accuracy: 9.48\n",
            "accuracy: 9.92\n",
            "accuracy: 9.82\n",
            "accuracy: 9.93\n",
            "accuracy: 10.06\n",
            "accuracy: 10.25\n",
            "accuracy: 9.99\n",
            "accuracy: 10.08\n",
            "accuracy: 10.00\n",
            "accuracy: 10.61\n",
            "accuracy: 9.97\n",
            "accuracy: 9.89\n",
            "accuracy: 10.11\n",
            "accuracy: 9.44\n",
            "accuracy: 9.54\n",
            "accuracy: 10.37\n",
            "accuracy: 9.87\n",
            "accuracy: 9.40\n",
            "accuracy: 10.13\n",
            "accuracy: 9.95\n",
            "accuracy: 10.28\n",
            "accuracy: 10.29\n",
            "accuracy: 10.22\n",
            "accuracy: 9.91\n",
            "accuracy: 9.72\n",
            "accuracy: 10.06\n",
            "accuracy: 10.00\n",
            "accuracy: 10.45\n",
            "accuracy: 10.44\n",
            "accuracy: 9.94\n",
            "accuracy: 10.56\n",
            "accuracy: 9.67\n",
            "accuracy: 10.12\n",
            "accuracy: 10.13\n",
            "accuracy: 10.46\n",
            "accuracy: 10.22\n",
            "accuracy: 9.94\n",
            "accuracy: 9.77\n",
            "accuracy: 10.56\n",
            "accuracy: 9.88\n",
            "accuracy: 9.80\n",
            "accuracy: 10.01\n",
            "accuracy: 9.20\n",
            "accuracy: 9.70\n",
            "accuracy: 9.74\n",
            "accuracy: 10.14\n",
            "accuracy: 10.43\n",
            "accuracy: 10.44\n",
            "accuracy: 10.52\n",
            "accuracy: 9.93\n",
            "accuracy: 10.09\n",
            "accuracy: 10.06\n",
            "accuracy: 10.19\n",
            "accuracy: 9.87\n",
            "accuracy: 9.74\n",
            "accuracy: 10.02\n",
            "accuracy: 10.12\n",
            "accuracy: 10.41\n",
            "accuracy: 10.45\n",
            "accuracy: 9.71\n",
            "accuracy: 9.82\n",
            "accuracy: 9.64\n",
            "accuracy: 9.81\n",
            "accuracy: 10.01\n",
            "accuracy: 9.91\n",
            "accuracy: 10.20\n"
          ]
        }
      ]
    }
  ]
}