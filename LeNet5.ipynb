{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeNet5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhSB6wBw2B3ifYzGH+TdeV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utkarsh-grow123/Deep-Learning/blob/main/LeNet5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code with initial layer with standard normalization and hidden layers with batch normalization"
      ],
      "metadata": {
        "id": "yvsKc5hPs8r4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "8I1Fc1stBLIy"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Module\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class Model(Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
        "\n",
        "        \n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, padding=0, stride=2)\n",
        "        \n",
        "        self.conv2=nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, padding=0, stride=2)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.bn3 = nn.BatchNorm1d(120)\n",
        "        self.bn4 = nn.BatchNorm1d(84)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.relu1(y)\n",
        "        y = self.pool1(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.bn2(y)\n",
        "        y = self.relu2(y)\n",
        "        y = self.pool2(y)\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        y = self.fc1(y)\n",
        "        y = self.bn3(y)\n",
        "        y = self.relu3(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.bn4(y)\n",
        "        y = self.relu4(y)\n",
        "        y = self.fc3(y)\n",
        "        y = self.relu5(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.datasets import mnist\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.2856], [0.3385]),\n",
        "])\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.2856], [0.3385]),\n",
        "])\n",
        "\n",
        "    trainset = mnist.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = mnist.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    model = Model()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "    epoch = 100\n",
        "\n",
        "    for _epoch in range(epoch):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss += loss.item()\n",
        "          _, predicted = outputs.max(1)\n",
        "          total += targets.size(0)\n",
        "          correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "      \n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct1 = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "          for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "              inputs, targets = inputs.to(device), targets.to(device)\n",
        "              loss = criterion(outputs, targets)\n",
        "\n",
        "              test_loss += loss.item()\n",
        "              _, predicted = outputs.max(1)\n",
        "              total += targets.size(0)\n",
        "              correct1 += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print('accuracy: {:.2f}'.format(correct1*100 / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CYD793kBX_F",
        "outputId": "3a5e0a5e-5587-42b3-df3a-caf2491a48fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 10.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code with input layer with batch normalisation as well as the hidden layers with batch normalisation"
      ],
      "metadata": {
        "id": "-NV6mmt8uRet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Module\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class ModelBatch(Module):\n",
        "    def __init__(self):\n",
        "        super(ModelBatch, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
        "\n",
        "        self.bn1=nn.BatchNorm2d(6)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, padding=0, stride=2)\n",
        "        \n",
        "        self.conv2=nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, padding=0, stride=2)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.bn3 = nn.BatchNorm1d(120)\n",
        "        self.bn4 = nn.BatchNorm1d(84)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.bn1(y)\n",
        "        y = self.relu1(y)\n",
        "        y = self.pool1(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.bn2(y)\n",
        "        y = self.relu2(y)\n",
        "        y = self.pool2(y)\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        y = self.fc1(y)\n",
        "        y = self.bn3(y)\n",
        "        y = self.relu3(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.bn4(y)\n",
        "        y = self.relu4(y)\n",
        "        y = self.fc3(y)\n",
        "        y = self.relu5(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "8yzuhQhbuSix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.datasets import mnist\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "    trainset = mnist.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = mnist.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    model = ModelBatch()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "    epoch = 100\n",
        "\n",
        "    for _epoch in range(epoch):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss += loss.item()\n",
        "          _, predicted = outputs.max(1)\n",
        "          total += targets.size(0)\n",
        "          correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "      \n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct1 = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "          for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "              inputs, targets = inputs.to(device), targets.to(device)\n",
        "              loss = criterion(outputs, targets)\n",
        "\n",
        "              test_loss += loss.item()\n",
        "              _, predicted = outputs.max(1)\n",
        "              total += targets.size(0)\n",
        "              correct1 += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print('accuracy: {:.2f}'.format(correct1*100 / total))"
      ],
      "metadata": {
        "id": "wKcFe5KTuhB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8W4RO8Peugdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code with dropout layer and wothout standard or batch normalization\n"
      ],
      "metadata": {
        "id": "ysJ2lv7RxrYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Module\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class ModelDropout(Module):\n",
        "    def __init__(self):\n",
        "        super(ModelDropout, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
        "\n",
        "        \n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, padding=0, stride=2)\n",
        "        \n",
        "        self.conv2=nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropoutConv = nn.Dropout(0.2)\n",
        "        self.dropoutHidden = nn.Dropout(0.5)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, padding=0, stride=2)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu5 = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.dropoutConv(y)\n",
        "        y = self.relu1(y)\n",
        "        y = self.pool1(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.dropoutHidden(y)\n",
        "        y = self.relu2(y)\n",
        "        y = self.pool2(y)\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        y = self.fc1(y)\n",
        "        y = self.dropoutHidden(y)\n",
        "        y = self.relu3(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.dropoutHidden(y)\n",
        "        y = self.relu4(y)\n",
        "        y = self.fc3(y)\n",
        "        y = self.relu5(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "CQP5P1KaxpXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.datasets import mnist\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "    trainset = mnist.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = mnist.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    model = ModelDropout()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "    epoch = 100\n",
        "\n",
        "    for _epoch in range(epoch):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss += loss.item()\n",
        "          _, predicted = outputs.max(1)\n",
        "          total += targets.size(0)\n",
        "          correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "      \n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct1 = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "          for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "              inputs, targets = inputs.to(device), targets.to(device)\n",
        "              loss = criterion(outputs, targets)\n",
        "\n",
        "              test_loss += loss.item()\n",
        "              _, predicted = outputs.max(1)\n",
        "              total += targets.size(0)\n",
        "              correct1 += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print('accuracy: {:.2f}'.format(correct1*100 / total))"
      ],
      "metadata": {
        "id": "9q6QMx-yx9Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "code with both dropout and batch normalization"
      ],
      "metadata": {
        "id": "tqWHI9vYArMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Module\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class ModelBatch(Module):\n",
        "    def __init__(self):\n",
        "        super(ModelBatch, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
        "\n",
        "        self.bn1=nn.BatchNorm2d(6)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, padding=0, stride=2)\n",
        "        self.dropoutConv = nn.Dropout(0.2)\n",
        "        self.dropoutHidden = nn.Dropout(0.5)\n",
        "        self.conv2=nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, padding=0, stride=2)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.bn3 = nn.BatchNorm1d(120)\n",
        "        self.bn4 = nn.BatchNorm1d(84)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.dropoutConv(y)\n",
        "        y = self.bn1(y)\n",
        "        y = self.relu1(y)\n",
        "        y = self.pool1(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.dropoutHidden(y)\n",
        "        y = self.bn2(y)\n",
        "        y = self.relu2(y)\n",
        "        y = self.pool2(y)\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        y = self.fc1(y)\n",
        "        y = self.dropoutHidden(y)\n",
        "        y = self.bn3(y)\n",
        "        y = self.relu3(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.dropoutHidden(y)\n",
        "        y = self.bn4(y)\n",
        "        y = self.relu4(y)\n",
        "        y = self.fc3(y)\n",
        "        y = self.relu5(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "z1KmrZMlA3nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.datasets import mnist\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "    trainset = mnist.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = mnist.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    model = ModelDropout()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "    epoch = 100\n",
        "\n",
        "    for _epoch in range(epoch):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss += loss.item()\n",
        "          _, predicted = outputs.max(1)\n",
        "          total += targets.size(0)\n",
        "          correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "      \n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct1 = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "          for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "              inputs, targets = inputs.to(device), targets.to(device)\n",
        "              loss = criterion(outputs, targets)\n",
        "\n",
        "              test_loss += loss.item()\n",
        "              _, predicted = outputs.max(1)\n",
        "              total += targets.size(0)\n",
        "              correct1 += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print('accuracy: {:.2f}'.format(correct1*100 / total))"
      ],
      "metadata": {
        "id": "-0Mma4x8CmzO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}